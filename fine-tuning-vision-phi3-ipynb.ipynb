{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1725397,"sourceType":"datasetVersion","datasetId":1023726}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Load Model \n","metadata":{}},{"cell_type":"code","source":"from PIL import Image \nimport requests \nfrom transformers import AutoModelForCausalLM ,AutoProcessor\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import AutoProcessor \nfrom huggingface_hub import login\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision.transforms.functional import resize, to_pil_image\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport os\n\n\nlogin(token=\"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\")\nmodel_id = \"microsoft/Phi-3-vision-128k-instruct\" \n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='eager') \nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \ntokenizer = processor.tokenizer\n\n\n# !pip install transformers  bitsandbytes accelerate\n# !pip install -U transformers\n# # !pip install flash_attn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-05T01:19:34.087852Z","iopub.execute_input":"2024-06-05T01:19:34.088223Z","iopub.status.idle":"2024-06-05T01:19:47.540365Z","shell.execute_reply.started":"2024-06-05T01:19:34.088196Z","shell.execute_reply":"2024-06-05T01:19:47.539534Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-05 01:19:36.665198: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-05 01:19:36.665262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-05 01:19:36.666850: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7017db1ea3c43e78e1098e7dec7c5ce"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## first testing model","metadata":{}},{"cell_type":"code","source":"def data_from_image(messages,image_source,max_new_tokens=500,temperature=0.0):\n    image = Image.open(requests.get(image_source, stream=True).raw) \n    \n    prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n    inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n\n    generation_args = { \n        \"max_new_tokens\": max_new_tokens, \n        \"temperature\": temperature, \n        \"do_sample\": False, \n    } \n\n    generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n\n    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n\n    print(response)\n    plt.imshow(image)\n    plt.axis(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = [ \n    {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is the emotion of person of image ?\"}, \n    {\"role\": \"assistant\", \"content\": \"I see He or she is  sad , no thing has importance to make any one sad realy thats life  \"}, \n    {\"role\": \"user\", \"content\": \"provide emotion of person in image ,Provide Methods to make it happy or exciting for life \"} \n] \nprompt_1='descripe image please and if its person tell me what is his face emotion like happy or sad '\nprompt_2='after prompt 1 how to make thats emotion positive?'\nmessages_template=f'<|user|>\\n<|image_1|>\\n{prompt_1}<|end|>\\n<|assistant|>\\n<|end|>\\n<|user|>\\n{prompt_2}<|end|>\\n<|assistant|>\\n'\nimage_source='https://th.bing.com/th/id/OIP.mnz9A0OgZQ0rSHNd77UrYAHaHa?rs=1&pid=ImgDetMain'\ndata_from_image(messages,image_source)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## preprocessing data and finetuning ","metadata":{}},{"cell_type":"code","source":"train_data=pd.read_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef load_train_val_test():\n    \n    # Defining path for the training,validation and testing datasets\n    \n    train_path = r'/kaggle/input/emotic/annots_arrs/annot_arrs_train.csv'\n    validation_path = r'/kaggle/input/emotic/annots_arrs/annot_arrs_val.csv'\n    test_path = r'/kaggle/input/emotic/annots_arrs/annot_arrs_test.csv'\n    # Importing the datasets\n    dataset = pd.DataFrame()\n    train_data = pd.read_csv(train_path)\n    val_data = pd.read_csv(validation_path)\n    test_data = pd.read_csv(test_path)\n    return train_data,val_data,test_data\n\n\ntrain_data,test_data,Validation_data=load_train_val_test()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize=np.load('/kaggle/input/emotic/img_arrs/crop_arr_train_0.npy')\nplt.imshow(visualize)\nplt.axis('off');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test","metadata":{}},{"cell_type":"code","source":"import cv2\nprompt_1='descripe image please and if its person tell me what is his face emotion like happy or sad '\nprompt_2='after prompt 1 how to make thats emotion positive?'\nmessages_template=f'<|user|>\\n<|image_1|>\\n{prompt_1}<|end|>\\n<|assistant|>\\n<|end|>\\n<|user|>\\n{prompt_2}<|end|>\\n<|assistant|>\\n'\nimage_source='https://th.bing.com/th/id/OIP.mnz9A0OgZQ0rSHNd77UrYAHaHa?rs=1&pid=ImgDetMain'\n\ndef data_from_image_local_link(messages,image_source,max_new_tokens=500,temperature=0.0):\n    image=np.load(image_source)\n    image = Image.fromarray(image)\n    prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n    inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n\n    generation_args = { \n        \"max_new_tokens\": max_new_tokens, \n        \"temperature\": temperature, \n        \"do_sample\": False, \n    } \n\n    generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n\n    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n\n    print(response)\n    plt.imshow(image)\n    plt.axis(False)\n\ndata_from_image_local_link(messages,'/kaggle/input/emotic/img_arrs/crop_arr_val_5.npy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_train_val_test()[0].isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(np.load(f\"/kaggle/input/emotic/img_arrs/{train_df['Filename'].iloc[0]}\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T01:19:29.275675Z","iopub.execute_input":"2024-06-05T01:19:29.276507Z","iopub.status.idle":"2024-06-05T01:19:29.605257Z","shell.execute_reply.started":"2024-06-05T01:19:29.276466Z","shell.execute_reply":"2024-06-05T01:19:29.603760Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/emotic/img_arrs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilename\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"],"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport random \nclass Emotic_Dataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length, image_size):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.tokenizer.padding_side = 'left'  # Set padding side to left\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        # Get the row at the given index\n        row = self.dataframe.iloc[idx]\n        emotion_values = row.iloc[5:33].values\n        # Create the text input for the model\n        emotion_index = np.argmax(emotion_values)\n        text = f\"\\nWhat is the emotion of person in image?\\n I see you : {row.iloc[emotion_index]}\"\n        \n        # Get the image path from the row\n        image_path = row['Filename']\n        image_path = os.path.join('/kaggle/input/emotic/img_arrs/',image_path)\n        \n        encodings = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length)\n        \n        try:\n            # Load and transform the image\n            image = Image.open(image_path).convert(\"RGB\")\n            image = self.image_transform_function(image)\n        except (FileNotFoundError, IOError):\n            # Skip the sample if the image is not found\n            return None\n        \n        # Add the image and price information to the encodings dictionary\n        encodings['pixel_values'] = image\n        encodings['price'] = row['full_price']\n        \n        return {key: torch.tensor(val) for key, val in encodings.items()}\n\n    def image_transform_function(self, image):\n        # Convert the image to a numpy array\n        image = np.array(image)\n        return image\n\n# Load dataset from disk\ntrain_df,test_df,val_df=load_train_val_test()\ntrain_dataset = Emotic_Dataset(train_df, tokenizer, max_length=512, image_size=128)\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n\n# Create dataset and dataloader for validation set\nval_dataset = Emotic_Dataset(val_df, tokenizer, max_length=512, image_size=128)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n# Set the device to GPU if available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Initialize the optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)\n\n# Training loop\nnum_epochs = 1\neval_interval = 150  # Evaluate every 'eval_interval' steps\nloss_scaling_factor = 1000.0  # Variable to scale the loss by a certain amount\nsave_dir = './saved_models'\nstep = 0\naccumulation_steps = 64  # Accumulate gradients over this many steps\n\n# Create a directory to save the best model\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\nbest_val_loss = float('inf')\nbest_model_path = None\n\n# Select 10 random images from the validation set for logging\nnum_log_samples = 10\nlog_indices = random.sample(range(len(val_dataset)), num_log_samples)\n\n# Function to extract the predicted price from model predictions\ndef extract_emotion_predictions(predictions, tokenizer):\n    # Assuming the price is at the end of the text and separated by a space\n    predicted_text = tokenizer.decode(predictions[0], skip_special_tokens=True)\n    try:\n        predicted_price = float(predicted_text.split()[-1].replace(',', ''))\n    except ValueError:\n        predicted_price = 0.0\n    return predicted_price\n\n# Function to evaluate the model on the validation set\ndef evaluate(model, val_loader, device, tokenizer, step, log_indices, max_samples=None):\n    model.eval()\n    total_loss = 0\n    total_price_error = 0\n    log_images = []\n    log_gt_texts = []\n    log_pred_texts = []\n    table = wandb.Table(columns=[\"Image\", \"Ground Truth Text\", \"Predicted Text\"])\n\n    with torch.no_grad():\n        for i, batch in enumerate(val_loader):\n            if max_samples and i >= max_samples:\n                break\n\n            if batch is None:  # Skip if the batch is None\n                continue\n\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            labels = input_ids.clone().detach()\n            actual_price = batch['price'].item()\n\n            outputs = model(\n                input_ids=input_ids, \n                attention_mask=attention_mask, \n                pixel_values=pixel_values, \n                labels=labels\n            )\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            # Calculate price error\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            predicted_price = extract_price_from_predictions(predictions, tokenizer)\n            price_error = abs(predicted_price - actual_price)\n            total_price_error += price_error\n\n            # Log images, ground truth texts, and predicted texts\n            if i in log_indices:\n                log_images.append(pixel_values.cpu().squeeze().numpy())\n                log_gt_texts.append(tokenizer.decode(labels[0], skip_special_tokens=True))\n                log_pred_texts.append(tokenizer.decode(predictions[0], skip_special_tokens=True))\n\n                # Convert image to PIL format\n                pil_img = to_pil_image(resize(torch.from_numpy(log_images[-1]).permute(2, 0, 1), (336, 336))).convert(\"RGB\")\n                \n                # Add data to the table\n                table.add_data(wandb.Image(pil_img), log_gt_texts[-1], log_pred_texts[-1])\n\n                # Log the table incrementally\n    wandb.log({\"Evaluation Results step {}\".format(step): table, \"Step\": step})\n\n    avg_loss = total_loss / (i + 1)  # i+1 to account for the loop index\n    avg_price_error = total_price_error / (i + 1)\n    model.train()\n\n    return avg_loss, avg_price_error\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.dataframe.isna().sum()\ntrain_dataset.__getitem__(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader.dataset[0]\nfor nth_batch, (batch_features, batch_labels) in enumerate(train_loader):\n    print(batch_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the model to training mode\nmodel.train()\n\n# Training loop for the specified number of epochs\nfor epoch in range(num_epochs):\n    total_train_loss = 0\n    total_train_price_error = 0\n    batch_count = 0\n\n    for batch in train_loader:\n        step += 1\n\n        if batch is None:  # Skip if the batch is None\n            continue\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        labels = input_ids.clone().detach()\n        actual_price = batch['price'].float().to(device)\n\n        outputs = model(\n            input_ids=input_ids, \n            attention_mask=attention_mask, \n            pixel_values=pixel_values, \n            labels=labels\n        )\n        loss = outputs.loss\n        total_loss = loss\n        predictions = torch.argmax(outputs.logits, dim=-1)            \n        predicted_price = extract_price_from_predictions(predictions, tokenizer)\n\n        total_loss.backward()\n\n        if (step % accumulation_steps) == 0:\n            for param in model.parameters():\n                if param.grad is not None:\n                    param.grad /= accumulation_steps\n            optimizer.step()\n            optimizer.zero_grad()\n\n        total_train_loss += total_loss.item()\n        total_train_price_error += abs(predicted_price - actual_price.item())\n        batch_count += 1\n\n        # Log batch loss to Weights & Biases\n        wandb.log({\"Batch Loss\": total_loss.item(), \"Step\": step})\n\n        print(f\"Epoch: {epoch}, Step: {step}, Batch Loss: {total_loss.item()}\")\n\n        if step % eval_interval == 0:\n            val_loss, val_price_error = evaluate(model, val_loader, device, tokenizer=tokenizer, log_indices=log_indices, step=step )\n            wandb.log({\n                \"Validation Loss\": val_loss,\n                \"Validation Price Error (Average)\": val_price_error,\n                \"Step\": step\n            })\n            print(f\"Step: {step}, Validation Loss: {val_loss}, Validation Price Error (Normalized): {val_price_error}\")\n\n            # Save the best model based on validation loss\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                best_model_path = os.path.join(save_dir, f\"best_model\")\n                model.save_pretrained(best_model_path, safe_serialization=False)\n                tokenizer.save_pretrained(best_model_path)\n\n            avg_train_loss = total_train_loss / batch_count\n            avg_train_price_error = total_train_price_error / batch_count\n            wandb.log({\n                \"Epoch\": epoch,\n                \"Average Training Loss\": avg_train_loss,\n                \"Average Training Price Error\": avg_train_price_error\n            })\n            \n    print(f\"Epoch: {epoch}, Average Training Loss: {avg_train_loss}, Average Training Price Error: {avg_train_price_error}\")\n\n    # Log the best model to Weights & Biases\n    if best_model_path:\n        run.log_model(\n            path=best_model_path,\n            name=\"phi3-v-burberry\",\n            aliases=[\"best\"],\n        )\n\n# Finish the Weights & Biases run\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}